Step-by-step plan
1. Figure out how to extract and manipulate data from the MNIST (Done)
2. Implement gradient descent (import from already completed python files) (Done)
3. Plan out general architecture for neural network (Done)
    a. Number of input units: dimensions of x(i)
    b. Number of output units: number of classes y(i)
    c. Number of hidden units per layer
    d. Default: 1 hidden layer, randomize initial parameters Theta
4. Implement forward propagation (Done)
5. Implement cost function (Done)
6. Implement back prop to compute partial derivatives (IP)
7. Gradient checking to check implementation of back prop
8. Gradient descent for optimizing the cost function

Reminder to split the test dataset into 50/50 CV/Test.

F-score history (F-score = 2PR/(P+R))

Work Log

To-do List:
- feature scaling

Feb 24
Project begins

Feb 25
Finished implementing load_data

Feb 28 

Attempt 1: using a single hidden layer with 29 units
            Matrix 1 - 2: 785 x 785    Matrix 2 - 3: 10 x 785              
x(i) (dim: 785 x 1) --> a1......a29 --> prediction for y(i) (dim 10 x 1)
Finished initialization of matrix and forward propagation, feature scaling

n = 785 too many features???

March 1

Proceed with current design. Reduce batch size to 200 -- executable amount.
- Fixed implementation detail (bias unit) in forward propagation
- Implemented cost function

March 2

Finished first implementation of backprop. Don't know whether it works or not because
gradient checking is too slow. 