Step-by-step plan
1. Figure out how to extract and manipulate data from the MNIST (Done)
2. Implement gradient descent (import from already completed python files) (Done)
3. Plan out general architecture for neural network (Done)
    a. Number of input units: dimensions of x(i)
    b. Number of output units: number of classes y(i)
    c. Number of hidden units per layer
    d. Default: 1 hidden layer, randomize initial parameters Theta
4. Implement forward propagation (Done)
5. Implement cost function (Done)
6. Implement back prop to compute partial derivatives (Done)
7. Gradient checking to check implementation of back prop (Done)
8. Gradient descent for optimizing the cost function (Done)

Reminder to split the test dataset into 50/50 CV/Test.

F-score history (F-score = 2PR/(P+R))

Work Log

To-do List:
- feature scaling

Feb 24
Project begins

Feb 25
Finished implementing load_data

Feb 28 

Attempt 1: using a single hidden layer with 29 units
            Matrix 1 - 2: 785 x 785    Matrix 2 - 3: 10 x 785              
x(i) (dim: 785 x 1) --> a1......a29 --> prediction for y(i) (dim 10 x 1)
Finished initialization of matrix and forward propagation, feature scaling

n = 785 too many features???

March 1

Proceed with current design. Reduce batch size to 200 -- executable amount.
- Fixed implementation detail (bias unit) in forward propagation
- Implemented cost function

March 2

Finished first implementation of backprop. Don't know whether it works or not because
gradient checking is too slow. 

March 3

Maybe backprop is working... No time to spend on this project today.
Figured out how to plot matrix as img.

Too many hidden units in the second layer. Computationally expensive and changing the value of 
one weight has virtually no effect:
    check = Theta[:]
    check[0][75][100] += 10
    print(J(Theta, train_img[0], train_label[0]))
    print(J(check, train_img[0], train_label[0]))
The above code produced exact same answers precise to 15 decimal places....
Too many weights ==> individual weights accounts for a neglegible amount of output.

New design:

785 features still
1 hidden layer
10 units + 1 bias unit

Theta size:
0: 10 * 785
1: 10 * 11

March 5

Change architecture.

Verified buggy implementation of backprop.

Debugging.

March 9

Debugging backprop continues.

Improved Forward Propagation and Cost function.

Fixed non-existent bug. Problem was with using only one set for gradient checking.

March 10

OK NEURAL NETWORK WORKS!!!
NOW I HAVE TO THINK ABOUT HOW TO TRAIN IT EFFICIENTLY!

March 22

Almost done. Doing a run on the test set for the final result.

March 23 

Project Done

May 17

Adding PCA to data to speed up processing time. Reduce dimensions.